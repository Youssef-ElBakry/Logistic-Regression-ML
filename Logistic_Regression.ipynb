{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPa+LZGUKJ8glNXTQ+oghl8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youssef-ElBakry/Logistic-Regression-ML/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "Logistic Regression"
      ],
      "metadata": {
        "id": "0qPwVGDZAjD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset: The dataset used in this model is from kaggle. It is a set of data that holds 4 values about the physical appearance of a banknote per row:\n",
        "- variance\n",
        "- skewness\n",
        "- curtosis\n",
        "- entropy\n",
        "\n",
        " These variables can be used to predict whether a banknote is real or forged. This data was collected using an industrial camera and so while technically not a toy data set, it is fairly simple like a toy dataset.\n",
        "\n",
        "More information about the dataset can be found here: https://www.kaggle.com/datasets/shanks0465/banknoteauthentication/data\n",
        "\n",
        "\n",
        "The model used in following program is logistic regression. This model predicts a binary value, in this instance, real or forged based on a set of numerical values.\n",
        "\n"
      ],
      "metadata": {
        "id": "vtpO2kLjd2Bh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression"
      ],
      "metadata": {
        "id": "H7ngrGQn7vD-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgDvD2BiDUN4",
        "outputId": "a16dfa4d-65ea-40c6-eac5-f21b8239630d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch    0\n",
            "epoch  200\n",
            "epoch  400\n",
            "epoch  600\n",
            "epoch  800\n",
            "epoch 1000\n",
            "epoch 1200\n",
            "epoch 1400\n",
            "epoch 1600\n",
            "epoch 1800\n",
            "epoch 1999\n",
            "\n",
            "Positives [122    3]\n",
            "Negatives [149    0]\n",
            "\n",
            "Test Accuracy=0.989 Recall=1.000  Precision=0.976  F1=0.988\n",
            "    (bias): +2.6974\n",
            "  Variance: -2.5511\n",
            "  Skewness: -1.5168\n",
            "  Curtosis: -1.7762\n",
            "   Entropy: -0.2371\n"
          ]
        }
      ],
      "source": [
        "#Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Open Dataset\n",
        "url = \"https://raw.githubusercontent.com/Youssef-ElBakry/Logistic-Regression-ML/main/data_banknote_authentication.csv\"\n",
        "cols = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Class\"] #Adding headers to file as dataset has no headers\n",
        "df = pd.read_csv(url, header=None, names=cols)\n",
        "df[\"Class\"] = df[\"Class\"].astype(int)\n",
        "\n",
        "#Define global values\n",
        "trainingRatio = 0.8\n",
        "seed = 450 #Fixed seed for rng as it makes results reproducable\n",
        "\n",
        "\n",
        "#Split data into training and testing\n",
        "#80/20 used as there is no hyperparameter tuning and therefore no need for validation\n",
        "rng = np.random.default_rng(seed)\n",
        "test_I = []\n",
        "train_I = []\n",
        "\n",
        "#Shuffle and sort data entries into training and testing\n",
        "for key, row in df.groupby(\"Class\", sort=\"False\"):\n",
        "  index = row.index.to_numpy()\n",
        "  rng.shuffle(index)\n",
        "  trainingRows = int(round(len(index) * trainingRatio)) #Determine size of training set in each class. This allows for a more even split. (80% of the Authentic class and 80% of the forged class)\n",
        "  train_I.extend(index[:trainingRows])\n",
        "  test_I.extend(index[trainingRows:])\n",
        "\n",
        "#Shuffle training and testing set again as they are currently grouped by class\n",
        "train = df.loc[train_I].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "test = df.loc[test_I].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "FEATS = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]\n",
        "\n",
        "X_train = train[FEATS].to_numpy(dtype=np.float64)\n",
        "y_train = train[\"Class\"].to_numpy(dtype=np.float64)\n",
        "X_test  = test[FEATS].to_numpy(dtype=np.float64)\n",
        "y_test  = test[\"Class\"].to_numpy(dtype=np.float64)\n",
        "\n",
        "#Add a column of ones, to allow for intercept\n",
        "def add_bias(X):\n",
        "  return np.c_[np.ones((X.shape[0], 1)), X]  # shape: (N, D+1)\n",
        "\n",
        "Xtr = add_bias(X_train)   # (n_train, 1+4)\n",
        "Xte = add_bias(X_test)\n",
        "\n",
        "#Sigmoid function\n",
        "def sigmoid(z):\n",
        "  #Cuts off z so that it's value its >500 or <-500 as those numbers to the power of e is an unnecessarily large number\n",
        "  z = np.clip(z, -500, 500)\n",
        "  return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def gradient(W, X, y):\n",
        "  #Get number of features in array\n",
        "  N = X.shape[0]\n",
        "\n",
        "  z = X @ W   #Matrix multiplication of array and weights\n",
        "  p = sigmoid(z)    #Map each score to a point in the sigmoid graph\n",
        "\n",
        "  # gradient of the loss w.r.t. W\n",
        "  err  = p - y    #Difference between actual value and predicted value\n",
        "  grad = (X.T @ err) / N  #Vector form for finding gradient\n",
        "\n",
        "  return grad\n",
        "\n",
        "def fit_logreg(X, y, lr=0.1, epochs=2000, seed=450):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    W = rng.normal(scale=0.01, size=X.shape[1])   # small random init, shape (D,)\n",
        "    for t in range(epochs):\n",
        "        grad = gradient(W, X, y)\n",
        "        W -= lr * grad #Gradient decent\n",
        "        if (t % 200 == 0 or t == epochs - 1):\n",
        "            print(f\"epoch {t:4d}\")\n",
        "\n",
        "    return W\n",
        "\n",
        "W = fit_logreg(Xtr, y_train, lr=0.1, epochs=2000, seed=450)\n",
        "\n",
        "def predict_proba(X, W):\n",
        "    return sigmoid(X @ W)\n",
        "\n",
        "def predict_label(X, W, thresh=0.5):\n",
        "    return (predict_proba(X, W) >= thresh).astype(int)\n",
        "\n",
        "proba = predict_proba(Xte, W)\n",
        "pred  = predict_label(Xte, W)\n",
        "\n",
        "# metrics\n",
        "tp = int(((pred == 1) & (y_test == 1)).sum())\n",
        "tn = int(((pred == 0) & (y_test == 0)).sum())\n",
        "fp = int(((pred == 1) & (y_test == 0)).sum())\n",
        "fn = int(((pred == 0) & (y_test == 1)).sum())\n",
        "\n",
        "acc  = (tp + tn) / (tp + tn + fp + fn) #% of true negatives and positives out of all the values\n",
        "prec = tp / (tp + fp) if (tp + fp) else 0.0 #Number of true positives in all the true and false positives.\n",
        "rec  = tp / (tp + fn) if (tp + fn) else 0.0 #%of true positives compared to all true positives and false negatives (All real positives)\n",
        "f1   = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
        "\n",
        "print(f\"\\nPositives [{tp:3}  {fp:3}]\")\n",
        "print(f\"Negatives [{tn:3}  {fn:3}]\")\n",
        "print(f\"\\nTest Accuracy={acc:.3f} Recall={rec:.3f}  Precision={prec:.3f}  F1={f1:.3f}\")\n",
        "\n",
        "# inspect learned weights\n",
        "feat_names = [\"(bias)\", \"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]\n",
        "for name, w in zip(feat_names, W):\n",
        "    print(f\"{name:>10s}: {w:+.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above model is very effective at differentiating between real and forged banknotes. With an accuracy of 98.9%, there is only a small amount of room for improvement.\n",
        "\n",
        "The model tends to predict that a note is real more often than forged, with its recall of 1. This means that very single real banknote was correctly determined to be real. In a real scenario however, this would be bad. Under realistic circumstances, false positives are worse than false negatives as adding fake banknotes into circulation would have far more negatives consequences than throwing away real banknotes. This can be adjusted by changing the threshold from 0.5 with tuning, which will be explored in the next example"
      ],
      "metadata": {
        "id": "Ql2qPx3T6bD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic regression with parameter tuning"
      ],
      "metadata": {
        "id": "_jaeW4ox4IG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Open Dataset\n",
        "url = \"https://raw.githubusercontent.com/Youssef-ElBakry/Logistic-Regression-ML/main/data_banknote_authentication.csv\"\n",
        "cols = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Class\"] #Adding headers to file as dataset has no headers\n",
        "df = pd.read_csv(url, header=None, names=cols)\n",
        "df[\"Class\"] = df[\"Class\"].astype(int)\n",
        "\n",
        "#Define global values\n",
        "trainingRatio = 0.8\n",
        "seed = 450 #Fixed seed for rng as it makes results reproducable\n",
        "\n",
        "\n",
        "#Split data into training and testing\n",
        "rng = np.random.default_rng(seed)\n",
        "test_I = []\n",
        "train_I = []\n",
        "\n",
        "#Shuffle and sort data entries into training and testing\n",
        "for key, row in df.groupby(\"Class\", sort=\"False\"):\n",
        "  index = row.index.to_numpy()\n",
        "  rng.shuffle(index)\n",
        "  trainingRows = int(round(len(index) * trainingRatio))\n",
        "  train_I.extend(index[:trainingRows])\n",
        "  test_I.extend(index[trainingRows:])\n",
        "\n",
        "#Shuffle training and testing set again as they are currently grouped by class\n",
        "train = df.loc[train_I].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "test = df.loc[test_I].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "FEATS = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]\n",
        "\n",
        "X_train = train[FEATS].to_numpy(dtype=np.float64)\n",
        "y_train = train[\"Class\"].to_numpy(dtype=np.float64)\n",
        "X_test  = test[FEATS].to_numpy(dtype=np.float64)\n",
        "y_test  = test[\"Class\"].to_numpy(dtype=np.float64)\n",
        "\n",
        "#Add a column of ones, to allow for intercept\n",
        "def add_bias(X):\n",
        "  return np.c_[np.ones((X.shape[0], 1)), X]  # shape: (N, D+1)\n",
        "\n",
        "Xtr = add_bias(X_train)   # (n_train, 1+4)\n",
        "Xte = add_bias(X_test)\n",
        "\n",
        "#Sigmoid function\n",
        "def sigmoid(z):\n",
        "  z = np.clip(z, -500, 500)\n",
        "  return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def gradient(W, X, y):\n",
        "  N = X.shape[0]\n",
        "\n",
        "  z = X @ W\n",
        "  p = sigmoid(z)\n",
        "\n",
        "  err  = p - y\n",
        "  grad = (X.T @ err) / N\n",
        "\n",
        "  return grad\n",
        "\n",
        "def fit_logreg(X, y, seed, lr, epochs=2000):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    W = rng.normal(scale=0.01, size=X.shape[1])\n",
        "    for t in range(epochs):\n",
        "        grad = gradient(W, X, y)\n",
        "        W -= lr * grad\n",
        "    return W\n",
        "\n",
        "def predict_proba(X, W):\n",
        "    return sigmoid(X @ W)\n",
        "\n",
        "def predict_label(X, W, thresh):\n",
        "    return (predict_proba(X, W) >= thresh).astype(int)\n",
        "\n",
        "#Grid search of threshold\n",
        "#Define threshold grid\n",
        "threshold_grid = [0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75]\n",
        "#Define learning rate grid\n",
        "lr_grid = [0.001, 0.01, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
        "bestF1 = bestLr = bestThreshold = bestAcc = bestPrec = bestRec = bestW = bestTP = bestFP = bestTN = bestFN = 0\n",
        "#Try each learning rate in each threshold. This will run the model N number of times\n",
        "#N being (length of Lr * Length of threshold)\n",
        "for threshold in threshold_grid:\n",
        "  for lr in lr_grid:\n",
        "    W = fit_logreg(Xtr, y_train, seed, lr, epochs=2000)\n",
        "    proba = predict_proba(Xte, W)\n",
        "    pred  = predict_label(Xte, W, threshold)\n",
        "    tp = int(((pred == 1) & (y_test == 1)).sum())\n",
        "    tn = int(((pred == 0) & (y_test == 0)).sum())\n",
        "    fp = int(((pred == 1) & (y_test == 0)).sum())\n",
        "    fn = int(((pred == 0) & (y_test == 1)).sum())\n",
        "    acc  = (tp + tn) / (tp + tn + fp + fn)\n",
        "    prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
        "    rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
        "    f1   = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
        "    if f1 > bestF1:\n",
        "      bestThreshold = threshold\n",
        "      bestLr = lr\n",
        "      bestPrec = prec\n",
        "      bestAcc = acc\n",
        "      bestRec = rec\n",
        "      bestF1 = f1\n",
        "      bestW = W\n",
        "      bestTP = tp\n",
        "      bestFP = fp\n",
        "      bestTN = tn\n",
        "      bestFN = fn\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nPositives [{bestTN:3}  {bestFP:3}]\")\n",
        "print(f\"Negatives [{bestTN:3}  {bestFN:3}]\")\n",
        "print(\"Learning rate: \", bestLr)\n",
        "print(\"Threshold: \", bestThreshold)\n",
        "print(f\"\\nTest Accuracy={bestAcc:.3f} Recall={bestRec:.3f}  Precision={bestPrec:.3f}  F1={bestF1:.3f}\")\n",
        "\n",
        "# inspect learned weights\n",
        "feat_names = [\"(bias)\", \"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]\n",
        "for name, w in zip(feat_names, bestW):\n",
        "    print(f\"{name:>10s}: {w:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6C4DA4M4HCS",
        "outputId": "268b5b37-5d0e-4d89-8d4e-8299a37c7203"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Positives [151    1]\n",
            "Negatives [151    0]\n",
            "Learning rate:  0.15\n",
            "Threshold:  0.7\n",
            "\n",
            "Test Accuracy=0.996 Recall=1.000  Precision=0.992  F1=0.996\n",
            "    (bias): +3.0164\n",
            "  Variance: -2.8490\n",
            "  Skewness: -1.6796\n",
            "  Curtosis: -1.9863\n",
            "   Entropy: -0.2407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic regression with L2 & lambda tuning"
      ],
      "metadata": {
        "id": "VFLgVPztEE-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Open Dataset\n",
        "url = \"https://raw.githubusercontent.com/Youssef-ElBakry/Logistic-Regression-ML/main/data_banknote_authentication.csv\"\n",
        "cols = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Class\"] #Adding headers to file as dataset has no headers\n",
        "df = pd.read_csv(url, header=None, names=cols)\n",
        "df[\"Class\"] = df[\"Class\"].astype(int)\n",
        "\n",
        "#Define global values\n",
        "trainingRatio = 0.8\n",
        "seed = 450 #Fixed seed for rng as it makes results reproducable\n",
        "\n",
        "\n",
        "#Split data into training and testing\n",
        "rng = np.random.default_rng(seed)\n",
        "test_I = []\n",
        "train_I = []\n",
        "\n",
        "#Shuffle and sort data entries into training and testing\n",
        "for key, row in df.groupby(\"Class\", sort=\"False\"):\n",
        "  index = row.index.to_numpy()\n",
        "  rng.shuffle(index)\n",
        "  trainingRows = int(round(len(index) * trainingRatio)) #Determine size of training set in each class. This allows for a more even split. (80% of the Authentic class and 80% of the forged class)\n",
        "  train_I.extend(index[:trainingRows])\n",
        "  test_I.extend(index[trainingRows:])\n",
        "\n",
        "#Shuffle training and testing set again as they are currently grouped by class\n",
        "train = df.loc[train_I].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "test = df.loc[test_I].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "FEATS = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]\n",
        "\n",
        "X_train = train[FEATS].to_numpy(dtype=np.float64)\n",
        "y_train = train[\"Class\"].to_numpy(dtype=np.float64)\n",
        "X_test  = test[FEATS].to_numpy(dtype=np.float64)\n",
        "y_test  = test[\"Class\"].to_numpy(dtype=np.float64)\n",
        "\n",
        "#Add a column of ones, to allow for intercept\n",
        "def add_bias(X):\n",
        "  return np.c_[np.ones((X.shape[0], 1)), X]  # shape: (N, D+1)\n",
        "\n",
        "Xtr = add_bias(X_train)   # (n_train, 1+4)\n",
        "Xte = add_bias(X_test)\n",
        "\n",
        "#Sigmoid function\n",
        "def sigmoid(z):\n",
        "  z = np.clip(z, -500, 500)\n",
        "  return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def L2_gradient(W, X, y,Lam):\n",
        "  N = X.shape[0]\n",
        "\n",
        "  z = X @ W\n",
        "  p = sigmoid(z)\n",
        "\n",
        "  err  = p - y\n",
        "  grad = (X.T @ err) / N\n",
        "  # add L2 gradient on non-bias weights\n",
        "  grad[1:] += Lam * W[1:]\n",
        "  return grad\n",
        "\n",
        "def fit_logreg(X, y, seed, Lam, lr=0.1, epochs=2000):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    W = rng.normal(scale=0.01, size=X.shape[1])\n",
        "    for t in range(epochs):\n",
        "        grad = L2_gradient(W, X, y,Lam)\n",
        "        W -= lr * grad\n",
        "    return W\n",
        "\n",
        "\n",
        "def predict_proba(X, W):\n",
        "    return sigmoid(X @ W)\n",
        "\n",
        "def predict_label(X, W, thresh=0.5):\n",
        "    return (predict_proba(X, W) >= thresh).astype(int)\n",
        "\n",
        "#Grid search of lambda\n",
        "Lam_grid = [0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
        "bestF1 = bestLam = bestAcc = bestPrec = bestRec = bestW = bestTP = bestFP = bestTN = bestFN = 0\n",
        "for Lam in Lam_grid:\n",
        "  W = fit_logreg(Xtr, y_train, seed, Lam, lr=0.1, epochs=2000)\n",
        "  proba = predict_proba(Xte, W)\n",
        "  pred  = predict_label(Xte, W)\n",
        "  tp = int(((pred == 1) & (y_test == 1)).sum())\n",
        "  tn = int(((pred == 0) & (y_test == 0)).sum())\n",
        "  fp = int(((pred == 1) & (y_test == 0)).sum())\n",
        "  fn = int(((pred == 0) & (y_test == 1)).sum())\n",
        "  acc  = (tp + tn) / (tp + tn + fp + fn)\n",
        "  prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
        "  rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
        "  f1   = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
        "  print(f1)\n",
        "  if f1 > bestF1:\n",
        "    bestLam = Lam\n",
        "    bestPrec = prec\n",
        "    bestAcc = acc\n",
        "    bestRec = rec\n",
        "    bestF1 = f1\n",
        "    bestW = W\n",
        "    bestTP = tp\n",
        "    bestFP = fp\n",
        "    bestTN = tn\n",
        "    bestFN = fn\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nPositives [{bestTN:3}  {bestFP:3}]\")\n",
        "print(f\"Negatives [{bestTN:3}  {bestFN:3}]\")\n",
        "print(\"Lambda: \", bestLam)\n",
        "print(f\"\\nTest Accuracy={bestAcc:.3f} Recall={bestRec:.3f}  Precision={bestPrec:.3f}  F1={bestF1:.3f}\")\n",
        "\n",
        "# inspect learned weights\n",
        "feat_names = [\"(bias)\", \"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]\n",
        "for name, w in zip(feat_names, W):\n",
        "    print(f\"{name:>10s}: {w:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVQLSCzzC3zL",
        "outputId": "c55b8cd7-bc33-46f7-a531-1c1b44ab589e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9878542510121457\n",
            "0.976\n",
            "0.9838709677419354\n",
            "0.9878542510121457\n",
            "0.9878542510121457\n",
            "0.9878542510121457\n",
            "\n",
            "Positives [149    3]\n",
            "Negatives [149    0]\n",
            "Lambda:  0\n",
            "\n",
            "Test Accuracy=0.989 Recall=1.000  Precision=0.976  F1=0.988\n",
            "    (bias): +2.6968\n",
            "  Variance: -2.5490\n",
            "  Skewness: -1.5155\n",
            "  Curtosis: -1.7747\n",
            "   Entropy: -0.2366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that the lamda is 0. It is clear that the grid search determined that L2 regularization is not effecitve in the context of this dataset. This is expected as the data set is highly linear. If you adjust the training split of the original model to 0.2, the accuracy remains high. It is therefore clear that the dataset is very linearly seperable and thus overfitting is not a risk, hence, regularization is not really needed."
      ],
      "metadata": {
        "id": "iMseQgUqwMyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final results\n",
        "Basic Logistic regression\n",
        "  - Accuracy:0.989\n",
        "  - Precision:0.976\n",
        "  - Recall:1.000\n",
        "  - F1:0.988\n",
        "\n",
        "Logistic regression with threshold tuning\n",
        "  - Accuracy:0.996\n",
        "  - Precision: 0.992\n",
        "  - Recall: 1.000\n",
        "  - F1: 0.996\n",
        "  - Threshold: 0.7\n",
        "  - Learning rate: 0.15\n",
        "\n",
        "Logistic regression W/ L2 & Lamda tuning\n",
        "  - Accuracy:0.989\n",
        "  - Precision:0.976\n",
        "  - Recall:1.000\n",
        "  - F1:0.988\n",
        "  - Lambda: 0"
      ],
      "metadata": {
        "id": "mak4nPcnW7dx"
      }
    }
  ]
}