{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPDMCj4P1oOIdJUO+fcEwhS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youssef-ElBakry/Logistic-Regression-ML/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "Logistic Regression"
      ],
      "metadata": {
        "id": "0qPwVGDZAjD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset: The dataset used in this model is from kaggle. It is a set of data that holds 4 values about the physical appearance of a banknote per row:\n",
        "- variance\n",
        "- skewness\n",
        "- curtosis\n",
        "- entropy\n",
        "\n",
        " These variables can be used to predict whether a banknote is real or forged. This data was collected using an industrial camera and so while technically not a toy data set, it is fairly simple like a toy dataset.\n",
        "\n",
        "More information about the dataset can be found here: https://www.kaggle.com/datasets/shanks0465/banknoteauthentication/data\n",
        "\n",
        "\n",
        "The model used in following program is logistic regression. This model predicts a binary value, in this instance, real or forged based on a set of numerical values.\n",
        "\n"
      ],
      "metadata": {
        "id": "vtpO2kLjd2Bh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression"
      ],
      "metadata": {
        "id": "H7ngrGQn7vD-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgDvD2BiDUN4",
        "outputId": "b7bba98f-2bf4-4e7b-a7c0-c767a2066416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classes: {0: 762, 1: 610}\n",
            "unique: [0 1]\n",
            "      Variance  Skewness  Curtosis  Entropy  Class\n",
            "0      3.62160   8.66610   -2.8073 -0.44699      0\n",
            "1      4.54590   8.16740   -2.4586 -1.46210      0\n",
            "2      3.86600  -2.63830    1.9242  0.10645      0\n",
            "3      3.45660   9.52280   -4.0112 -3.59440      0\n",
            "4      0.32924  -4.45520    4.5718 -0.98880      0\n",
            "...        ...       ...       ...      ...    ...\n",
            "1367   0.40614   1.34920   -1.4501 -0.55949      1\n",
            "1368  -1.38870  -4.87730    6.4774  0.34179      1\n",
            "1369  -3.75030 -13.45860   17.5932 -2.77710      1\n",
            "1370  -3.56370  -8.38270   12.3930 -1.28230      1\n",
            "1371  -2.54190  -0.65804    2.6842  1.19520      1\n",
            "\n",
            "[1372 rows x 5 columns]\n",
            "\n",
            "Positives [122    5]\n",
            "Negatives [147    0]\n",
            "\n",
            "Test Accuracy=0.982 Recall=1.000  Precision=0.961  F1=0.980\n",
            "    (bias): -1.0178\n",
            "  Variance: -4.2103\n",
            "  Skewness: -3.8551\n",
            "  Curtosis: -3.6189\n",
            "   Entropy: +0.2054\n"
          ]
        }
      ],
      "source": [
        "#Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Open Dataset\n",
        "url = \"https://raw.githubusercontent.com/Youssef-ElBakry/Logistic-Regression-ML/main/data_banknote_authentication.csv\"\n",
        "cols = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Class\"] #Adding headers to file as dataset has no headers\n",
        "df = pd.read_csv(url, header=None, names=cols)\n",
        "df[\"Class\"] = df[\"Class\"].astype(int)\n",
        "print(\"classes:\", df[\"Class\"].value_counts().to_dict())\n",
        "print(\"unique:\", df[\"Class\"].unique())\n",
        "print(df)\n",
        "\n",
        "#Define global values\n",
        "trainingRatio = 0.8\n",
        "LabelCol = \"LABEL\"\n",
        "seed = 450 #Fixed seed for rng as it makes results reproducable\n",
        "\n",
        "\n",
        "#Split data into training and testing\n",
        "#80/20 used as there is no hyperparameter tuning and therefore no need for validation\n",
        "rng = np.random.default_rng(seed)\n",
        "test_I = []\n",
        "train_I = []\n",
        "\n",
        "#Shuffle and sort data entries into training and testing\n",
        "for key, row in df.groupby(\"Class\", sort=\"False\"):\n",
        "  index = row.index.to_numpy()\n",
        "  rng.shuffle(index)\n",
        "  trainingRows = int(round(len(index) * trainingRatio)) #Determine size of training set in each class. This allows for a more even split. (80% of the Authentic class and 80% of the forged class)\n",
        "  train_I.extend(index[:trainingRows])\n",
        "  test_I.extend(index[trainingRows:])\n",
        "\n",
        "#Shuffle training and testing set again as they are currently grouped by class\n",
        "train = df.loc[train_I].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "test = df.loc[test_I].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "FEATS = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]\n",
        "\n",
        "X_train = train[FEATS].to_numpy(dtype=np.float64)\n",
        "y_train = train[\"Class\"].to_numpy(dtype=np.float64)\n",
        "X_test  = test[FEATS].to_numpy(dtype=np.float64)\n",
        "y_test  = test[\"Class\"].to_numpy(dtype=np.float64)\n",
        "\n",
        "#Standardise data using mean and standardisation to find z-scores\n",
        "mu = X_train.mean(axis=0) #Calculate mean of each column\n",
        "sd = X_train.std(axis=0) #Calculate standard deviation for each column\n",
        "\n",
        "X_train_std = (X_train - mu) / sd #Turn each value in each column to a zscore\n",
        "X_test_std = (X_test - mu) / sd\n",
        "\n",
        "#Add a column of ones, to allow for intercept\n",
        "def add_bias(X):\n",
        "  return np.c_[np.ones((X.shape[0], 1)), X]  # shape: (N, D+1)\n",
        "\n",
        "Xtr = add_bias(X_train_std)   # (n_train, 1+4)\n",
        "Xte = add_bias(X_test_std)\n",
        "\n",
        "#Sigmoid function\n",
        "def sigmoid(z):\n",
        "  #Cuts off z so that it's value its >500 or <-500 as those numbers to the power of e is an unnecessarily large number\n",
        "  z = np.clip(z, -500, 500)\n",
        "  return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def logloss_and_grad(W, X, y):\n",
        "  #Get number of features in array\n",
        "  N = X.shape[0]\n",
        "\n",
        "  z = X @ W   #Matrix multiplication of array and weights\n",
        "  p = sigmoid(z)    #Map each score to a point in the sigmoid graph\n",
        "\n",
        "  # log loss (cross-entropy)\n",
        "  eps = 1e-12   #avoid log(0) by setting adding a very small number\n",
        "  loss = -np.mean(y*np.log(p + eps) + (1 - y)*np.log(1 - p + eps))\n",
        "\n",
        "  # gradient of the loss w.r.t. W\n",
        "  err  = p - y    #Difference between actual value and predicted value\n",
        "  grad = (X.T @ err) / N  #Vector form for finding gradient\n",
        "\n",
        "  return loss, grad\n",
        "\n",
        "def fit_logreg(X, y, lr=0.1, epochs=2000, seed=450):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    W = rng.normal(scale=0.01, size=X.shape[1])   # small random init, shape (D,)\n",
        "    for t in range(epochs):\n",
        "        loss, grad = logloss_and_grad(W, X, y)\n",
        "        W -= lr * grad #Gradient decent\n",
        "        if (t % 200 == 0 or t == epochs - 1):\n",
        "            print(f\"epoch {t:4d}  loss={loss:.4f}\")\n",
        "        if not np.isfinite(loss):\n",
        "            raise RuntimeError(\"Loss exploded/NaN. Lower lr (e.g., 0.05 or 0.01).\")\n",
        "    return W\n",
        "\n",
        "W = fit_logreg(Xtr, y_train, lr=0.1, epochs=2000, seed=450)\n",
        "\n",
        "def predict_proba(X, W):\n",
        "    return sigmoid(X @ W)\n",
        "\n",
        "def predict_label(X, W, thresh=0.5):\n",
        "    return (predict_proba(X, W) >= thresh).astype(int)\n",
        "\n",
        "proba = predict_proba(Xte, W)\n",
        "pred  = predict_label(Xte, W)\n",
        "\n",
        "# metrics\n",
        "tp = int(((pred == 1) & (y_test == 1)).sum())\n",
        "tn = int(((pred == 0) & (y_test == 0)).sum())\n",
        "fp = int(((pred == 1) & (y_test == 0)).sum())\n",
        "fn = int(((pred == 0) & (y_test == 1)).sum())\n",
        "\n",
        "acc  = (tp + tn) / (tp + tn + fp + fn) #% of true negatives and positives out of all the values\n",
        "prec = tp / (tp + fp) if (tp + fp) else 0.0 #Number of true positives in all the true and false positives.\n",
        "rec  = tp / (tp + fn) if (tp + fn) else 0.0 #%of true positives compared to all true positives and false negatives (All real positives)\n",
        "f1   = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
        "\n",
        "print(f\"\\nPositives [{tp:3}  {fp:3}]\")\n",
        "print(f\"Negatives [{tn:3}  {fn:3}]\")\n",
        "print(f\"\\nTest Accuracy={acc:.3f} Recall={rec:.3f}  Precision={prec:.3f}  F1={f1:.3f}\")\n",
        "\n",
        "# inspect learned weights\n",
        "feat_names = [\"(bias)\", \"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]\n",
        "for name, w in zip(feat_names, W):\n",
        "    print(f\"{name:>10s}: {w:+.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above model is very effective at differentiating between real and forged banknotes. With an accuracy of 98.2%, there is only a small amount of room for improvement.\n",
        "\n",
        "The model tends to predict that a note is real more often than forged, with its recall of 1. This means that very single real banknote was correctly determined to be real. In a real scenario however, this would be bad. Under realistic circumstances, false positives are worse than false negatives as adding fake banknotes into circulation would have far more negatives consequences than throwing away real banknotes. This can be adjusted by changing the threshold from 0.5. In a following example, hyperparameter tuning will be used to adjust the threshold to find a more accurate value\n",
        "\n",
        "Important notes:\n",
        "While accuracy is high, it is a flawed measure. If one of the two outcomes are rare, and the model chooses the common one for all outcomes, the accuracy will appear to be high. For this reason, the precision and recall value exists."
      ],
      "metadata": {
        "id": "Ql2qPx3T6bD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic regression with L1"
      ],
      "metadata": {
        "id": "vdbra3_76hGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Open Dataset\n",
        "url = \"https://raw.githubusercontent.com/Youssef-ElBakry/Logistic-Regression-ML/main/data_banknote_authentication.csv\"\n",
        "cols = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Class\"] #Adding headers to file as dataset has no headers\n",
        "df = pd.read_csv(url, header=None, names=cols)\n",
        "df[\"Class\"] = df[\"Class\"].astype(int)\n",
        "print(\"classes:\", df[\"Class\"].value_counts().to_dict())\n",
        "print(\"unique:\", df[\"Class\"].unique())\n",
        "print(df)\n",
        "\n",
        "#Define global values\n",
        "trainingRatio = 0.8\n",
        "LabelCol = \"LABEL\"\n",
        "seed = 450 #Fixed seed for rng as it makes results reproducable\n",
        "\n",
        "\n",
        "#Split data into training and testing\n",
        "#80/20 used as there is no hyperparameter tuning and therefore no need for validation\n",
        "rng = np.random.default_rng(seed)\n",
        "test_I = []\n",
        "train_I = []\n",
        "\n",
        "#Shuffle and sort data entries into training and testing\n",
        "for key, row in df.groupby(\"Class\", sort=\"False\"):\n",
        "  index = row.index.to_numpy()\n",
        "  rng.shuffle(index)\n",
        "  trainingRows = int(round(len(index) * trainingRatio)) #Determine size of training set in each class. This allows for a more even split. (80% of the Authentic class and 80% of the forged class)\n",
        "  train_I.extend(index[:trainingRows])\n",
        "  test_I.extend(index[trainingRows:])\n",
        "\n",
        "#Shuffle training and testing set again as they are currently grouped by class\n",
        "train = df.loc[train_I].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "test = df.loc[test_I].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "FEATS = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]\n",
        "\n",
        "X_train = train[FEATS].to_numpy(dtype=np.float64)\n",
        "y_train = train[\"Class\"].to_numpy(dtype=np.float64)\n",
        "X_test  = test[FEATS].to_numpy(dtype=np.float64)\n",
        "y_test  = test[\"Class\"].to_numpy(dtype=np.float64)\n",
        "\n",
        "#Standardise data using mean and standardisation to find z-scores\n",
        "mu = X_train.mean(axis=0) #Calculate mean of each column\n",
        "sd = X_train.std(axis=0) #Calculate standard deviation for each column\n",
        "\n",
        "X_train_std = (X_train - mu) / sd #Turn each value in each column to a zscore\n",
        "X_test_std = (X_test - mu) / sd\n",
        "\n",
        "#Add a column of ones, to allow for intercept\n",
        "def add_bias(X):\n",
        "  return np.c_[np.ones((X.shape[0], 1)), X]  # shape: (N, D+1)\n",
        "\n",
        "Xtr = add_bias(X_train_std)   # (n_train, 1+4)\n",
        "Xte = add_bias(X_test_std)\n",
        "\n",
        "#Sigmoid function\n",
        "def sigmoid(z):\n",
        "  #Cuts off z so that it's value its >500 or <-500 as those numbers to the power of e is an unnecessarily large number\n",
        "  z = np.clip(z, -500, 500)\n",
        "  return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def logloss_and_grad(W, X, y):\n",
        "  #Get number of features in array\n",
        "  N = X.shape[0]\n",
        "\n",
        "  z = X @ W   #Matrix multiplication of array and weights\n",
        "  p = sigmoid(z)    #Map each score to a point in the sigmoid graph\n",
        "\n",
        "  # log loss (cross-entropy)\n",
        "  eps = 1e-12   #avoid log(0) by setting adding a very small number\n",
        "  loss = -np.mean(y*np.log(p + eps) + (1 - y)*np.log(1 - p + eps))\n",
        "\n",
        "  # gradient of the loss w.r.t. W\n",
        "  err  = p - y    #Difference between actual value and predicted value\n",
        "  grad = (X.T @ err) / N  #Vector form for finding gradient\n",
        "\n",
        "  return loss, grad\n",
        "\n",
        "def l1_loss_no_bias(W, lam):\n",
        "  # Do NOT regularize the bias term\n",
        "  return lam * np.sum(np.abs(W[1:]))\n",
        "\n",
        "def soft_threshold(v, tau):\n",
        "    # Prox operator for L1: shrink toward zero, possibly to exactly zero\n",
        "    return np.sign(v) * np.maximum(np.abs(v) - tau, 0.0)\n",
        "\n",
        "def fit_logreg_L1(X, y, lr=0.1, epochs=2000, lam=1e-3, seed=450):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    W = rng.normal(scale=0.01, size=X.shape[1])\n",
        "\n",
        "    for t in range(epochs):\n",
        "        loss, grad = logloss_and_grad(W, X, y)\n",
        "\n",
        "        #Gradient step on the smooth part\n",
        "        W -= lr * grad\n",
        "\n",
        "        # 3) Prox step for L1 on the non-bias weights\n",
        "        W[1:] = soft_threshold(W[1:], lr * lam)\n",
        "\n",
        "        if t % 200 == 0 or t == epochs - 1:\n",
        "            total = loss + l1_loss_no_bias(W, lam)\n",
        "            nnz = int(np.count_nonzero(np.abs(W[1:]) > 0))\n",
        "            print(f\"epoch {t:4d}  logloss={loss:.4f}  + L1={total:.4f}  nnz={nnz}\")\n",
        "\n",
        "        if not np.isfinite(loss):\n",
        "            raise RuntimeError(\"Loss exploded/NaN. Lower lr (e.g., 0.05 or 0.01).\")\n",
        "\n",
        "    return W\n",
        "\n",
        "W = fit_logreg_L1(Xtr, y_train, lr=0.1, epochs=2000, lam=1e-3, seed=450)\n",
        "\n",
        "def predict_proba(X, W):\n",
        "    return sigmoid(X @ W)\n",
        "\n",
        "def predict_label(X, W, thresh=0.5):\n",
        "    return (predict_proba(X, W) >= thresh).astype(int)\n",
        "\n",
        "proba = predict_proba(Xte, W)\n",
        "pred  = predict_label(Xte, W)\n",
        "\n",
        "# metrics\n",
        "tp = int(((pred == 1) & (y_test == 1)).sum())\n",
        "tn = int(((pred == 0) & (y_test == 0)).sum())\n",
        "fp = int(((pred == 1) & (y_test == 0)).sum())\n",
        "fn = int(((pred == 0) & (y_test == 1)).sum())\n",
        "\n",
        "acc  = (tp + tn) / (tp + tn + fp + fn) #% of true negatives and positives out of all the values\n",
        "prec = tp / (tp + fp) if (tp + fp) else 0.0 #Number of true positives in all the true and false positives.\n",
        "rec  = tp / (tp + fn) if (tp + fn) else 0.0 #%of true positives compared to all true positives and false negatives (All real positives)\n",
        "f1   = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
        "\n",
        "print(f\"\\nPositives [{tp:3}  {fp:3}]\")\n",
        "print(f\"Negatives [{tn:3}  {fn:3}]\")\n",
        "print(f\"\\nTest Accuracy={acc:.3f} Recall={rec:.3f}  Precision={prec:.3f}  F1={f1:.3f}\")\n",
        "\n",
        "# inspect learned weights\n",
        "feat_names = [\"(bias)\", \"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]\n",
        "for name, w in zip(feat_names, W):\n",
        "    print(f\"{name:>10s}: {w:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aETkUTwV6f26",
        "outputId": "69c31c7a-7b29-44f0-8de8-4e28ab6db6df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classes: {0: 762, 1: 610}\n",
            "unique: [0 1]\n",
            "      Variance  Skewness  Curtosis  Entropy  Class\n",
            "0      3.62160   8.66610   -2.8073 -0.44699      0\n",
            "1      4.54590   8.16740   -2.4586 -1.46210      0\n",
            "2      3.86600  -2.63830    1.9242  0.10645      0\n",
            "3      3.45660   9.52280   -4.0112 -3.59440      0\n",
            "4      0.32924  -4.45520    4.5718 -0.98880      0\n",
            "...        ...       ...       ...      ...    ...\n",
            "1367   0.40614   1.34920   -1.4501 -0.55949      1\n",
            "1368  -1.38870  -4.87730    6.4774  0.34179      1\n",
            "1369  -3.75030 -13.45860   17.5932 -2.77710      1\n",
            "1370  -3.56370  -8.38270   12.3930 -1.28230      1\n",
            "1371  -2.54190  -0.65804    2.6842  1.19520      1\n",
            "\n",
            "[1372 rows x 5 columns]\n",
            "epoch    0  logloss=0.6940  + L1=0.6940  nnz=4\n",
            "epoch  200  logloss=0.1985  + L1=0.2029  nnz=4\n",
            "epoch  400  logloss=0.1340  + L1=0.1404  nnz=4\n",
            "epoch  600  logloss=0.1068  + L1=0.1144  nnz=4\n",
            "epoch  800  logloss=0.0918  + L1=0.1003  nnz=4\n",
            "epoch 1000  logloss=0.0822  + L1=0.0915  nnz=4\n",
            "epoch 1200  logloss=0.0755  + L1=0.0854  nnz=4\n",
            "epoch 1400  logloss=0.0705  + L1=0.0809  nnz=4\n",
            "epoch 1600  logloss=0.0667  + L1=0.0775  nnz=4\n",
            "epoch 1800  logloss=0.0636  + L1=0.0747  nnz=4\n",
            "epoch 1999  logloss=0.0610  + L1=0.0725  nnz=4\n",
            "\n",
            "Positives [122    6]\n",
            "Negatives [146    0]\n",
            "\n",
            "Test Accuracy=0.978 Recall=1.000  Precision=0.953  F1=0.976\n",
            "    (bias): -0.9896\n",
            "  Variance: -4.1032\n",
            "  Skewness: -3.7629\n",
            "  Curtosis: -3.5101\n",
            "   Entropy: +0.1648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic regression with L2"
      ],
      "metadata": {
        "id": "KJz6w_fvH2dP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Open Dataset\n",
        "url = \"https://raw.githubusercontent.com/Youssef-ElBakry/Logistic-Regression-ML/main/data_banknote_authentication.csv\"\n",
        "cols = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Class\"] #Adding headers to file as dataset has no headers\n",
        "df = pd.read_csv(url, header=None, names=cols)\n",
        "df[\"Class\"] = df[\"Class\"].astype(int)\n",
        "print(\"classes:\", df[\"Class\"].value_counts().to_dict())\n",
        "print(\"unique:\", df[\"Class\"].unique())\n",
        "print(df)\n",
        "\n",
        "#Define global values\n",
        "trainingRatio = 0.8\n",
        "LabelCol = \"LABEL\"\n",
        "seed = 450 #Fixed seed for rng as it makes results reproducable\n",
        "\n",
        "\n",
        "#Split data into training and testing\n",
        "#80/20 used as there is no hyperparameter tuning and therefore no need for validation\n",
        "rng = np.random.default_rng(seed)\n",
        "test_I = []\n",
        "train_I = []\n",
        "\n",
        "#Shuffle and sort data entries into training and testing\n",
        "for key, row in df.groupby(\"Class\", sort=\"False\"):\n",
        "  index = row.index.to_numpy()\n",
        "  rng.shuffle(index)\n",
        "  trainingRows = int(round(len(index) * trainingRatio)) #Determine size of training set in each class. This allows for a more even split. (80% of the Authentic class and 80% of the forged class)\n",
        "  train_I.extend(index[:trainingRows])\n",
        "  test_I.extend(index[trainingRows:])\n",
        "\n",
        "#Shuffle training and testing set again as they are currently grouped by class\n",
        "train = df.loc[train_I].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "test = df.loc[test_I].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "FEATS = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]\n",
        "\n",
        "X_train = train[FEATS].to_numpy(dtype=np.float64)\n",
        "y_train = train[\"Class\"].to_numpy(dtype=np.float64)\n",
        "X_test  = test[FEATS].to_numpy(dtype=np.float64)\n",
        "y_test  = test[\"Class\"].to_numpy(dtype=np.float64)\n",
        "\n",
        "#Standardise data using mean and standardisation to find z-scores\n",
        "mu = X_train.mean(axis=0) #Calculate mean of each column\n",
        "sd = X_train.std(axis=0) #Calculate standard deviation for each column\n",
        "\n",
        "X_train_std = (X_train - mu) / sd #Turn each value in each column to a zscore\n",
        "X_test_std = (X_test - mu) / sd\n",
        "\n",
        "#Add a column of ones, to allow for intercept\n",
        "def add_bias(X):\n",
        "  return np.c_[np.ones((X.shape[0], 1)), X]  # shape: (N, D+1)\n",
        "\n",
        "Xtr = add_bias(X_train_std)   # (n_train, 1+4)\n",
        "Xte = add_bias(X_test_std)\n",
        "\n",
        "#Sigmoid function\n",
        "def sigmoid(z):\n",
        "  #Cuts off z so that it's value its >500 or <-500 as those numbers to the power of e is an unnecessarily large number\n",
        "  z = np.clip(z, -500, 500)\n",
        "  return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def logloss_L2_and_grad(W, X, y,Lam):\n",
        "  #Get number of features in array\n",
        "  N = X.shape[0]\n",
        "\n",
        "  z = X @ W   #Matrix multiplication of array and weights\n",
        "  p = sigmoid(z)    #Map each score to a point in the sigmoid graph\n",
        "\n",
        "  # log loss (cross-entropy)\n",
        "  eps = 1e-12   #avoid log(0) by setting adding a very small number\n",
        "  loss = -np.mean(y*np.log(p + eps) + (1 - y)*np.log(1 - p + eps))\n",
        "\n",
        "  # L2 penalty (exclude bias W[0])\n",
        "  l2 = 0.5 * Lam * np.sum(W[1:]**2)\n",
        "\n",
        "  err  = p - y\n",
        "  grad = (X.T @ err) / N\n",
        "  # add L2 gradient on non-bias weights\n",
        "  g = grad.copy()\n",
        "  g[1:] += Lam * W[1:]\n",
        "  return loss + l2, g\n",
        "\n",
        "def fit_logreg(X, y, lr=0.1, epochs=2000, Lam=1e-3, seed=450):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    W = rng.normal(scale=0.01, size=X.shape[1])   # small random init, shape (D,)\n",
        "    for t in range(epochs):\n",
        "        loss, grad = logloss_L2_and_grad(W, X, y,Lam)\n",
        "        W -= lr * grad #Gradient decent\n",
        "        if (t % 200 == 0 or t == epochs - 1):\n",
        "            print(f\"epoch {t:4d}  loss={loss:.4f}\")\n",
        "        if not np.isfinite(loss):\n",
        "            raise RuntimeError(\"Loss exploded/NaN. Lower lr (e.g., 0.05 or 0.01).\")\n",
        "    return W\n",
        "\n",
        "W = fit_logreg(Xtr, y_train, lr=0.1, epochs=2000, Lam=1e-3, seed=450)\n",
        "\n",
        "def predict_proba(X, W):\n",
        "    return sigmoid(X @ W)\n",
        "\n",
        "def predict_label(X, W, thresh=0.5):\n",
        "    return (predict_proba(X, W) >= thresh).astype(int)\n",
        "\n",
        "proba = predict_proba(Xte, W)\n",
        "pred  = predict_label(Xte, W)\n",
        "\n",
        "# metrics\n",
        "tp = int(((pred == 1) & (y_test == 1)).sum())\n",
        "tn = int(((pred == 0) & (y_test == 0)).sum())\n",
        "fp = int(((pred == 1) & (y_test == 0)).sum())\n",
        "fn = int(((pred == 0) & (y_test == 1)).sum())\n",
        "\n",
        "acc  = (tp + tn) / (tp + tn + fp + fn) #% of true negatives and positives out of all the values\n",
        "prec = tp / (tp + fp) if (tp + fp) else 0.0 #Number of true positives in all the true and false positives.\n",
        "rec  = tp / (tp + fn) if (tp + fn) else 0.0 #%of true positives compared to all true positives and false negatives (All real positives)\n",
        "f1   = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
        "\n",
        "print(f\"\\nPositives [{tp:3}  {fp:3}]\")\n",
        "print(f\"Negatives [{tn:3}  {fn:3}]\")\n",
        "print(f\"\\nTest Accuracy={acc:.3f} Recall={rec:.3f}  Precision={prec:.3f}  F1={f1:.3f}\")\n",
        "\n",
        "# inspect learned weights\n",
        "feat_names = [\"(bias)\", \"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]\n",
        "for name, w in zip(feat_names, W):\n",
        "    print(f\"{name:>10s}: {w:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x5SGX1wlniJ",
        "outputId": "aa63c2ed-4e29-4711-a594-30d001bfb7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classes: {0: 762, 1: 610}\n",
            "unique: [0 1]\n",
            "      Variance  Skewness  Curtosis  Entropy  Class\n",
            "0      3.62160   8.66610   -2.8073 -0.44699      0\n",
            "1      4.54590   8.16740   -2.4586 -1.46210      0\n",
            "2      3.86600  -2.63830    1.9242  0.10645      0\n",
            "3      3.45660   9.52280   -4.0112 -3.59440      0\n",
            "4      0.32924  -4.45520    4.5718 -0.98880      0\n",
            "...        ...       ...       ...      ...    ...\n",
            "1367   0.40614   1.34920   -1.4501 -0.55949      1\n",
            "1368  -1.38870  -4.87730    6.4774  0.34179      1\n",
            "1369  -3.75030 -13.45860   17.5932 -2.77710      1\n",
            "1370  -3.56370  -8.38270   12.3930 -1.28230      1\n",
            "1371  -2.54190  -0.65804    2.6842  1.19520      1\n",
            "\n",
            "[1372 rows x 5 columns]\n",
            "epoch    0  loss=0.6940\n",
            "epoch  200  loss=0.2020\n",
            "epoch  400  loss=0.1416\n",
            "epoch  600  loss=0.1178\n",
            "epoch  800  loss=0.1055\n",
            "epoch 1000  loss=0.0982\n",
            "epoch 1200  loss=0.0935\n",
            "epoch 1400  loss=0.0902\n",
            "epoch 1600  loss=0.0879\n",
            "epoch 1800  loss=0.0862\n",
            "epoch 1999  loss=0.0849\n",
            "0.976\n",
            "\n",
            "Positives [122    6]\n",
            "Negatives [146    0]\n",
            "\n",
            "Test Accuracy=0.978 Recall=1.000  Precision=0.953  F1=0.976\n",
            "    (bias): -0.9268\n",
            "  Variance: -3.9102\n",
            "  Skewness: -3.5477\n",
            "  Curtosis: -3.3106\n",
            "   Entropy: +0.1787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic regression with L2 & Lambda tuning"
      ],
      "metadata": {
        "id": "irg7x-3vCzSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Open Dataset\n",
        "url = \"https://raw.githubusercontent.com/Youssef-ElBakry/Logistic-Regression-ML/main/data_banknote_authentication.csv\"\n",
        "cols = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Class\"] #Adding headers to file as dataset has no headers\n",
        "df = pd.read_csv(url, header=None, names=cols)\n",
        "df[\"Class\"] = df[\"Class\"].astype(int)\n",
        "print(\"classes:\", df[\"Class\"].value_counts().to_dict())\n",
        "print(\"unique:\", df[\"Class\"].unique())\n",
        "print(df)\n",
        "\n",
        "#Define global values\n",
        "trainingRatio = 0.8\n",
        "LabelCol = \"LABEL\"\n",
        "seed = 450 #Fixed seed for rng as it makes results reproducable\n",
        "\n",
        "\n",
        "#Split data into training and testing\n",
        "#80/20 used as there is no hyperparameter tuning and therefore no need for validation\n",
        "rng = np.random.default_rng(seed)\n",
        "test_I = []\n",
        "train_I = []\n",
        "\n",
        "#Shuffle and sort data entries into training and testing\n",
        "for key, row in df.groupby(\"Class\", sort=\"False\"):\n",
        "  index = row.index.to_numpy()\n",
        "  rng.shuffle(index)\n",
        "  trainingRows = int(round(len(index) * trainingRatio)) #Determine size of training set in each class. This allows for a more even split. (80% of the Authentic class and 80% of the forged class)\n",
        "  train_I.extend(index[:trainingRows])\n",
        "  test_I.extend(index[trainingRows:])\n",
        "\n",
        "#Shuffle training and testing set again as they are currently grouped by class\n",
        "train = df.loc[train_I].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "test = df.loc[test_I].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "FEATS = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]\n",
        "\n",
        "X_train = train[FEATS].to_numpy(dtype=np.float64)\n",
        "y_train = train[\"Class\"].to_numpy(dtype=np.float64)\n",
        "X_test  = test[FEATS].to_numpy(dtype=np.float64)\n",
        "y_test  = test[\"Class\"].to_numpy(dtype=np.float64)\n",
        "\n",
        "#Standardise data using mean and standardisation to find z-scores\n",
        "mu = X_train.mean(axis=0) #Calculate mean of each column\n",
        "sd = X_train.std(axis=0) #Calculate standard deviation for each column\n",
        "\n",
        "X_train_std = (X_train - mu) / sd #Turn each value in each column to a zscore\n",
        "X_test_std = (X_test - mu) / sd\n",
        "\n",
        "#Add a column of ones, to allow for intercept\n",
        "def add_bias(X):\n",
        "  return np.c_[np.ones((X.shape[0], 1)), X]  # shape: (N, D+1)\n",
        "\n",
        "Xtr = add_bias(X_train_std)   # (n_train, 1+4)\n",
        "Xte = add_bias(X_test_std)\n",
        "\n",
        "#Sigmoid function\n",
        "def sigmoid(z):\n",
        "  #Cuts off z so that it's value its >500 or <-500 as those numbers to the power of e is an unnecessarily large number\n",
        "  z = np.clip(z, -500, 500)\n",
        "  return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def logloss_L2_and_grad(W, X, y,Lam):\n",
        "  #Get number of features in array\n",
        "  N = X.shape[0]\n",
        "\n",
        "  z = X @ W   #Matrix multiplication of array and weights\n",
        "  p = sigmoid(z)    #Map each score to a point in the sigmoid graph\n",
        "\n",
        "  # log loss (cross-entropy)\n",
        "  eps = 1e-12   #avoid log(0) by setting adding a very small number\n",
        "  loss = -np.mean(y*np.log(p + eps) + (1 - y)*np.log(1 - p + eps))\n",
        "\n",
        "  # L2 penalty (exclude bias W[0])\n",
        "  l2 = 0.5 * Lam * np.sum(W[1:]**2)\n",
        "\n",
        "  err  = p - y\n",
        "  grad = (X.T @ err) / N\n",
        "  # add L2 gradient on non-bias weights\n",
        "  g = grad.copy()\n",
        "  g[1:] += Lam * W[1:]\n",
        "  return loss + l2, g\n",
        "\n",
        "def fit_logreg(X, y, seed, Lam, lr=0.1, epochs=2000):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    W = rng.normal(scale=0.01, size=X.shape[1])   # small random init, shape (D,)\n",
        "    for t in range(epochs):\n",
        "        loss, grad = logloss_L2_and_grad(W, X, y,Lam)\n",
        "        W -= lr * grad #Gradient decent\n",
        "        if not np.isfinite(loss):\n",
        "            raise RuntimeError(\"Loss exploded/NaN. Lower lr (e.g., 0.05 or 0.01).\")\n",
        "    return W\n",
        "\n",
        "\n",
        "def predict_proba(X, W):\n",
        "    return sigmoid(X @ W)\n",
        "\n",
        "def predict_label(X, W, thresh=0.5):\n",
        "    return (predict_proba(X, W) >= thresh).astype(int)\n",
        "\n",
        "Lam_grid = [0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
        "bestF1 = bestLam = bestAcc = bestPrec = bestRec = bestW = 0\n",
        "for Lam in Lam_grid:\n",
        "  W = fit_logreg(Xtr, y_train, seed, Lam, lr=0.1, epochs=2000)\n",
        "  proba = predict_proba(Xte, W)\n",
        "  pred  = predict_label(Xte, W)\n",
        "  tp = int(((pred == 1) & (y_test == 1)).sum())\n",
        "  tn = int(((pred == 0) & (y_test == 0)).sum())\n",
        "  fp = int(((pred == 1) & (y_test == 0)).sum())\n",
        "  fn = int(((pred == 0) & (y_test == 1)).sum())\n",
        "  acc  = (tp + tn) / (tp + tn + fp + fn)\n",
        "  prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
        "  rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
        "  f1   = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
        "  print(f1)\n",
        "  if f1 > bestF1:\n",
        "    bestLam = Lam\n",
        "    bestPrec = prec\n",
        "    bestAcc = acc\n",
        "    bestRec = rec\n",
        "    bestF1 = f1\n",
        "    bestW = W\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nPositives [{tp:3}  {fp:3}]\")\n",
        "print(f\"Negatives [{tn:3}  {fn:3}]\")\n",
        "print(Lam)\n",
        "print(f\"\\nTest Accuracy={bestAcc:.3f} Recall={bestRec:.3f}  Precision={bestPrec:.3f}  F1={bestF1:.3f}\")\n",
        "\n",
        "# inspect learned weights\n",
        "feat_names = [\"(bias)\", \"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]\n",
        "for name, w in zip(feat_names, W):\n",
        "    print(f\"{name:>10s}: {w:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVQLSCzzC3zL",
        "outputId": "743e8ab4-5eca-4f8c-f892-8b16bacf19da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classes: {0: 762, 1: 610}\n",
            "unique: [0 1]\n",
            "      Variance  Skewness  Curtosis  Entropy  Class\n",
            "0      3.62160   8.66610   -2.8073 -0.44699      0\n",
            "1      4.54590   8.16740   -2.4586 -1.46210      0\n",
            "2      3.86600  -2.63830    1.9242  0.10645      0\n",
            "3      3.45660   9.52280   -4.0112 -3.59440      0\n",
            "4      0.32924  -4.45520    4.5718 -0.98880      0\n",
            "...        ...       ...       ...      ...    ...\n",
            "1367   0.40614   1.34920   -1.4501 -0.55949      1\n",
            "1368  -1.38870  -4.87730    6.4774  0.34179      1\n",
            "1369  -3.75030 -13.45860   17.5932 -2.77710      1\n",
            "1370  -3.56370  -8.38270   12.3930 -1.28230      1\n",
            "1371  -2.54190  -0.65804    2.6842  1.19520      1\n",
            "\n",
            "[1372 rows x 5 columns]\n",
            "0.9799196787148594\n",
            "0.9344262295081968\n",
            "0.9641434262948207\n",
            "0.976\n",
            "0.9799196787148594\n",
            "0.9799196787148594\n",
            "\n",
            "Positives [122    5]\n",
            "Negatives [147    0]\n",
            "1e-05\n",
            "\n",
            "Test Accuracy=0.982 Recall=1.000  Precision=0.961  F1=0.980\n",
            "    (bias): -1.0168\n",
            "  Variance: -4.2070\n",
            "  Skewness: -3.8517\n",
            "  Curtosis: -3.6155\n",
            "   Entropy: +0.2051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final results\n",
        "Basic Logistic regression\n",
        "  - Accuracy:0.982\n",
        "  - Precision:0.961\n",
        "  - Recall:1.000\n",
        "  - F1:0.980\n",
        "\n",
        "Logistic regression W/ L1\n",
        "  - Accuracy: 0.978\n",
        "  - Precision:0.953\n",
        "  - Recall: 1.000\n",
        "  - F1: 0.976\n",
        "\n",
        "Logistic regression W/ L2\n",
        "  - Accuracy: 0.978\n",
        "  - Precision: 0.953  \n",
        "  - Recall: 1.000\n",
        "  - F1: 0.976\n",
        "\n",
        "Logistic regression W/ L2 & Lamda tuning\n",
        "  - Accuracy: 0.982\n",
        "  - Precision: 0.961\n",
        "  - Recall: 1.000\n",
        "  - F1: 0.980\n"
      ],
      "metadata": {
        "id": "mak4nPcnW7dx"
      }
    }
  ]
}