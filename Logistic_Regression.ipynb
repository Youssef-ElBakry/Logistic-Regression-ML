{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNU3jrCJ2Ih9frOMAFkoJXz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youssef-ElBakry/Logistic-Regression-ML/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "Logistic Regression"
      ],
      "metadata": {
        "id": "0qPwVGDZAjD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset: The dataset used in this model is from kaggle. It is a set of data that holds 4 values about the physical appearance of a banknote per row:\n",
        "- variance\n",
        "- skewness\n",
        "- curtosis\n",
        "- entropy\n",
        "\n",
        " These variables can be used to predict whether a banknote is real or forged. This data was collected using an industrial camera and so while technically not a toy data set, it is fairly simple like a toy dataset.\n",
        "\n",
        "More information about the dataset can be found here: https://www.kaggle.com/datasets/shanks0465/banknoteauthentication/data\n",
        "\n",
        "\n",
        "The model used in following program is logistic regression. This model predicts a binary value, in this instance, real or forged based on a set of numerical values.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vtpO2kLjd2Bh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgDvD2BiDUN4",
        "outputId": "ea11dd31-80dc-4ac3-9a9f-d989a720ecf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classes: {0: 762, 1: 610}\n",
            "unique: [0 1]\n",
            "      Variance  Skewness  Curtosis  Entropy  Class\n",
            "0      3.62160   8.66610   -2.8073 -0.44699      0\n",
            "1      4.54590   8.16740   -2.4586 -1.46210      0\n",
            "2      3.86600  -2.63830    1.9242  0.10645      0\n",
            "3      3.45660   9.52280   -4.0112 -3.59440      0\n",
            "4      0.32924  -4.45520    4.5718 -0.98880      0\n",
            "...        ...       ...       ...      ...    ...\n",
            "1367   0.40614   1.34920   -1.4501 -0.55949      1\n",
            "1368  -1.38870  -4.87730    6.4774  0.34179      1\n",
            "1369  -3.75030 -13.45860   17.5932 -2.77710      1\n",
            "1370  -3.56370  -8.38270   12.3930 -1.28230      1\n",
            "1371  -2.54190  -0.65804    2.6842  1.19520      1\n",
            "\n",
            "[1372 rows x 5 columns]\n",
            "epoch    0  loss=0.6940\n",
            "epoch  200  loss=0.1969\n",
            "epoch  400  loss=0.1322\n",
            "epoch  600  loss=0.1050\n",
            "epoch  800  loss=0.0899\n",
            "epoch 1000  loss=0.0803\n",
            "epoch 1200  loss=0.0736\n",
            "epoch 1400  loss=0.0686\n",
            "epoch 1600  loss=0.0647\n",
            "epoch 1800  loss=0.0616\n",
            "epoch 1999  loss=0.0590\n",
            "\n",
            "Test Accuracy=0.982  Precision=0.961  Recall=1.000  F1=0.980\n",
            "    (bias): -1.0178\n",
            "  Variance: -4.2103\n",
            "  Skewness: -3.8551\n",
            "  Curtosis: -3.6189\n",
            "   Entropy: +0.2054\n"
          ]
        }
      ],
      "source": [
        "#Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Open Dataset\n",
        "url = \"https://raw.githubusercontent.com/Youssef-ElBakry/Logistic-Regression-ML/main/data_banknote_authentication.csv\"\n",
        "cols = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Class\"] #Adding headers to file as dataset has no headers\n",
        "df = pd.read_csv(url, header=None, names=cols)\n",
        "df[\"Class\"] = df[\"Class\"].astype(int)\n",
        "print(\"classes:\", df[\"Class\"].value_counts().to_dict())\n",
        "print(\"unique:\", df[\"Class\"].unique())\n",
        "print(df)\n",
        "\n",
        "#Define global values\n",
        "trainingRatio = 0.8\n",
        "LabelCol = \"LABEL\"\n",
        "seed = 450 #Fixed seed for rng as it makes results reproducable\n",
        "\n",
        "\n",
        "#Split data into training and testing\n",
        "#80/20 used as there is no hyperparameter tuning and therefore no need for validation\n",
        "rng = np.random.default_rng(seed)\n",
        "test_I = []\n",
        "train_I = []\n",
        "\n",
        "#Shuffle and sort data entries into training and testing\n",
        "for key, row in df.groupby(\"Class\", sort=\"False\"):\n",
        "  index = row.index.to_numpy()\n",
        "  rng.shuffle(index)\n",
        "  trainingRows = int(round(len(index) * trainingRatio)) #Determine size of training set in each class. This allows for a more even split. (80% of the spam class and 80% of the ham class)\n",
        "  train_I.extend(index[:trainingRows])\n",
        "  test_I.extend(index[trainingRows:])\n",
        "\n",
        "#Shuffle training and testing set again as they are currently grouped by class\n",
        "train = df.loc[train_I].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "test = df.loc[test_I].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "FEATS = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]\n",
        "\n",
        "X_train = train[FEATS].to_numpy(dtype=np.float64)\n",
        "y_train = train[\"Class\"].to_numpy(dtype=np.float64)\n",
        "X_test  = test[FEATS].to_numpy(dtype=np.float64)\n",
        "y_test  = test[\"Class\"].to_numpy(dtype=np.float64)\n",
        "\n",
        "#Standardise data using mean and standardisation to find z-scores\n",
        "mu = X_train.mean(axis=0) #Calculate mean of each column\n",
        "sd = X_train.std(axis=0) #Calculate standard deviation for each column\n",
        "\n",
        "X_train_std = (X_train - mu) / sd #Turn each value in each column to a zscore\n",
        "X_test_std = (X_test - mu) / sd\n",
        "\n",
        "#Add a column of ones, to allow for intercept\n",
        "def add_bias(X):\n",
        "  return np.c_[np.ones((X.shape[0], 1)), X]  # shape: (N, D+1)\n",
        "\n",
        "Xtr = add_bias(X_train_std)   # (n_train, 1+4)\n",
        "Xte = add_bias(X_test_std)\n",
        "\n",
        "#Sigmoid function\n",
        "def sigmoid(z):\n",
        "  #Cuts off z so that it's value its >500 or <-500 as those numbers to the power of e is an unnecessarily large number\n",
        "  z = np.clip(z, -500, 500)\n",
        "  return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def logloss_and_grad(W, X, y):\n",
        "  #Get number of features in array\n",
        "  N = X.shape[0]\n",
        "\n",
        "  z = X @ W   #Matrix multiplication of array and weights\n",
        "  p = sigmoid(z)    #Map each score to a point in the sigmoid graph\n",
        "\n",
        "  # log loss (cross-entropy)\n",
        "  eps = 1e-12   #avoid log(0) by setting adding a very small number\n",
        "  loss = -np.mean(y*np.log(p + eps) + (1 - y)*np.log(1 - p + eps))\n",
        "\n",
        "  # gradient of the loss w.r.t. W\n",
        "  err  = p - y    #Difference between actual value and predicted value\n",
        "  grad = (X.T @ err) / N  #Vector form for finding gradient/average error\n",
        "\n",
        "  return loss, grad\n",
        "\n",
        "def fit_logreg(X, y, lr=0.1, epochs=2000, verbose=True, seed=450):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    W = rng.normal(scale=0.01, size=X.shape[1])   # small random init, shape (D,)\n",
        "    for t in range(epochs):\n",
        "        loss, grad = logloss_and_grad(W, X, y)\n",
        "        W -= lr * grad #Gradient decent\n",
        "        if verbose and (t % 200 == 0 or t == epochs - 1):\n",
        "            print(f\"epoch {t:4d}  loss={loss:.4f}\")\n",
        "        if not np.isfinite(loss):\n",
        "            raise RuntimeError(\"Loss exploded/NaN. Lower lr (e.g., 0.05 or 0.01).\")\n",
        "    return W\n",
        "\n",
        "W = fit_logreg(Xtr, y_train, lr=0.1, epochs=2000, verbose=True, seed=450)\n",
        "\n",
        "def predict_proba(X, W):\n",
        "    return sigmoid(X @ W)\n",
        "\n",
        "def predict_label(X, W, thresh=0.5):\n",
        "    return (predict_proba(X, W) >= thresh).astype(int)\n",
        "\n",
        "proba = predict_proba(Xte, W)\n",
        "pred  = predict_label(Xte, W)\n",
        "\n",
        "# metrics (binary)\n",
        "tp = int(((pred == 1) & (y_test == 1)).sum())\n",
        "tn = int(((pred == 0) & (y_test == 0)).sum())\n",
        "fp = int(((pred == 1) & (y_test == 0)).sum())\n",
        "fn = int(((pred == 0) & (y_test == 1)).sum())\n",
        "\n",
        "acc  = (tp + tn) / (tp + tn + fp + fn)\n",
        "prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
        "rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
        "f1   = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
        "\n",
        "print(f\"\\nTest Accuracy={acc:.3f}  Precision={prec:.3f}  Recall={rec:.3f}  F1={f1:.3f}\")\n",
        "\n",
        "# inspect learned weights\n",
        "feat_names = [\"(bias)\", \"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]\n",
        "for name, w in zip(feat_names, W):\n",
        "    print(f\"{name:>10s}: {w:+.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PnMH2XO1buAf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}